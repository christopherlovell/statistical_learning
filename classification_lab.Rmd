---
title: "Classification Lab"
author: "Christopher Lovell"
date: "Monday, March 23, 2015"
output: html_document
---

```{r}
library(ISLR)

```

```{r}
names(Smarket)
summary(Smarket)
```

`cor()` function produces a matrix containing all the pairwise correlations among the predictors. We ignore the qualitative `Direction` variable. 
```{r}
cor(Smarket[,-9])
```

Little correlation between lags (a hint of greater corrlation between consecutive days), but a high correlation between volume and year. Can see this if we plot volume - increases with time.
```{r}
attach(Smarket)
plot(Volume)
```

Now fit a logistic regression to predict `Direction` using the lag and volume predictors. Use the `glm()` function, which is similar to the `lm` function used in linear regression, but we pass the `family=binomial` argument to indicate that we wish to run a logistic regression rather than a generalized linear model.
```{r}
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag5+Volume,data=Smarket,family = binomial)
summary(glm.fit)
coef(glm.fit)
```

p values are pretty large, but the smallest is for `Lag1`, suggesting that the previous days return is negatively correlated with todays return, i.e. if the market went up yesterday, it's less likely to go up again today.

To get a prediction use the `predict()` function. Passing `type="response"` as an argument returns probabilities of the form $P(Y=1|X)$. If no data is provided probabilities against the training data are returned. 
```{r}
glm.probs<-predict(glm.fit,type="response")
glm.probs[1:10]
```

But which value is a positive or negative prediction? The `contrasts` function tells us which dummy variables have been assigned.
```{r}
contrasts(Direction)
```

How to evaluate the model? We can create a data frame of prediction outcomes (set any value above 0.5 to 'Up'), then compare this with the Direction variable in a *confusion matrix*.
```{r}
glm.pred=rep("Down",1250)
glm.pred[glm.probs>.5]<-"Up"

table(glm.pred,Direction)
mean(glm.pred==Direction)
```

This error rate is misleading as it is tested on the training data. Going to create a training and test data set and compare error rate.
```{r}
train=(Year<2005)
Smarket.2005=Smarket[!train,]
dim(Smarket.2005)
dim(subset(Smarket,subset=train))
Direction.2005=Direction[!train]

glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag5+Volume,data=Smarket,family = binomial,subset=train)
glm.probs=predict(glm.fit,Smarket.2005,type="response")
```

```{r}
glm.pred=rep("Down",252)
glm.pred[glm.probs>.5]<-"Up"

table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
```

Now we see that the model has a higher than 50% error rate on the test set.

How can we improve it? Start by removing those predictors with high p values.
```{r}
glm.fit=glm(Direction~Lag1+Lag2,data=Smarket,family = binomial,subset=train)
glm.probs=predict(glm.fit,Smarket.2005,type="response")
```
```{r}
glm.pred=rep("Down",252)
glm.pred[glm.probs>.5]<-"Up"

table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
```

Slightly better.

How about some fresh data?
```{r}
predict(glm.fit,newdata = data.frame(Lag1=c(1.2,1.5),Lag2=c(1.1,-0.8)),type="response")
```

