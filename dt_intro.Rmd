---
title: "Tree Based Methods"
output: html_document
---

## Classification Trees

Load tree library for creating our decision tree models. Load ISLR library for the `Carseats` data set. Replace the sales continous variable with a binary variable.
```{r}
library(tree)

library(ISLR)
attach(Carseats)
High <- ifelse(Sales<=8,"No","Yes")
```

Create a data frame with the new binary variable.
```{r}
Carseats <- data.frame(Carseats,High)
detach(Carseats)
```

Create a decision tree using all variables except the original continous `Sales` variable. 
```{r}
tree.Carseats <- tree(High~.-Sales, Carseats)
summary(tree.Carseats)
```

Plot the tree with labels.
```{r}
plot(tree.Carseats)
text(tree.Carseats,pretty=0)
```

Summaries the tree.
```{r}
tree.Carseats
```

Create train and test data sets to allow evaluation of test error.
```{r}
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[-train,]
High.test = High[-train]

tree.Carseats <- tree(High~.-Sales, Carseats, subset = train)
tree.pred <- predict(tree.Carseats, Carseats.test, type="class")

test.results <- table(tree.pred,High.test)
test.results
paste("Test error rate: ",100*(1-sum(diag(test.results))/length(train)),"%",sep="")
```

Next, consider whether pruning the tree improves results. Specify `prune.misclass` so that classification error used to guide cross-validation and pruning process. `dev` in `cv.carseat` is actually the misclassification error rate; lowest for `size=9` tree.
```{r}
set.seed(3)

cv.carseat <- cv.tree(tree.Carseats, FUN = prune.misclass)
names(cv.carseat)
cv.carseat
```

`k` corresponds to the cross-complexity error.
```{r}
par(mfrow=c(1,2))
plot(cv.carseat$size, cv.carseat$dev, type="b")
plot(cv.carseat$k, cv.carseat$dev, type="b")
```

Now prune, to obtain nine node tree.
```{r}
prune.carseats <- prune.misclass(tree.Carseats, best = 9)
par(mfrow=c(1,1))
plot(prune.carseats)
text(prune.carseats,pretty = 0)
```

```{r}
tree.pred <- predict(prune.carseats, Carseats.test, type="class")

test.results <- table(tree.pred,High.test)
test.results
paste("Test error rate: ",100*(1-sum(diag(test.results))/length(train)),"%",sep="")
```


## Regression Trees

Fit a regression tree to the `Boston` data set.
```{r}
library(MASS)
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston)/2)

tree.boston <- tree(medv~., Boston, subset = train)
summary(tree.boston)
```

Can see from `summary` that only three variables are actually used in the tree.

```{r}
plot(tree.boston)
text(tree.boston,pretty = 0)
```

Cross validation selects the most complex tree.

```{r}
cv.boston <- cv.tree(tree.boston)
par(mfrow=c(1,2))
plot(cv.boston$size, cv.boston$dev, type="b")
plot(cv.boston$size, cv.boston$k, type="b")
```

If we did wish to prune the tree anyway, would do so as follows.

```{r}
prune.boston <- prune.tree(tree.boston, best = 5)
par(mfrow=c(1,1))
plot(prune.boston)
text(prune.boston, pretty = 0)
```

```{r}
yhat <- predict(tree.boston, newdata = Boston[-train,])
boston.test <- Boston[-train,"medv"]

plot(yhat,boston.test)
abline(0,1)
paste("Test set MSE associated with regression tree: ", mean((yhat-boston.test)^2))
```


## Bagging and Random Forests

Bagging example. `mtry` states that all 13 predictors should be considered for each split of the tree.

```{r}
library(randomForest)
set.seed(1)
bag.boston <- randomForest(medv~., data = Boston, subset = train, mtry = 13, importance = T)
bag.boston
```

```{r}
yhat.bag <- predict(bag.boston, newdata = Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
paste("Test set MSE associated with bagged tree: ", mean((yhat.bag-boston.test)^2))
```

Change number of trees grown using the `ntree` argument.

```{r}
bag.boston <- randomForest(medv~., data = Boston, subset = train, mtry = 13, ntree = 25)
yhat.bag <- predict(bag.boston, newdata = Boston[-train,])
paste("Test set MSE associated with bagged tree: ", mean((yhat.bag-boston.test)^2))
```

Growing a random forest proceeds in the same way, but with a smaller value for `mtry`.

```{r}
set.seed(1)
rf.boston <- randomForest(medv~., data = Boston, subset = train, mtry = 6, importance = T)
yhat.rf <- predict(rf.boston, newdata = Boston[-train,])
paste("Test set MSE associated with random forest: ", mean((yhat.rf-boston.test)^2))
```

Can use `importance` function to view importance of variables used.
```{r}
importance(rf.boston)
varImpPlot(rf.boston)
```


## Boosting

Boosting example. Plot shows relative importance. Use a gaussian distribution as this is a regression problem; ir performing classification, us a bernoulli distribution. `interaction.depth` limits depth of each tree.

```{r}
library(gbm)
set.seed(1)
boost.boston <- gbm(medv~., data = Boston[-train,], distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
summary(boost.boston)
```

Can also produce *partial dependence plots*, which integrate out other variables to show the marginal effect of selected variables. Can see that median house prices are increasing with `rm`, and decreasing with `lstat`.

```{r}
par(mfrow=c(1,2))
plot(boost.boston, i="rm")
plot(boost.boston, i="lstat")
```

```{r}
yhat.boost <- predict(boost.boston, newdata = Boston[-train,], n.trees = 5000)
paste("Test set MSE associated with random forest: ", mean((yhat.boost-boston.test)^2))
```


Set our shrinkage term manually to $\lambda=0.2$.
```{r}
boost.boston <- gbm(medv~., data = Boston[train,], distribution = "gaussian", n.trees = 5000, interaction.depth = 4, shrinkage = 0.2, verbose = F)
yhat.boost <- predict(boost.boston, newdata = Boston[-train,], n.trees = 5000)
paste("Test set MSE associated with random forest: ", mean((yhat.boost-boston.test)^2))
```







