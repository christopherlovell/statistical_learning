---
title: "Tree Based Methods"
output: html_document
---

A guide to using tree based methods in R. Based on the relevant chapter in 'Introduction to Statistical Learning', including my own additional points on prettier plotting, and using other packages.

## Classification Trees
For when you want to predict a qualitative response. 

I'll use the `tree` library to create my decision tree models. The ISLR library contains the `Carseats` data set. Use the sales continous variable to create a binary variable.
```{r}
library(tree)
library(ISLR)
Carseats$High <- as.factor(ifelse(Carseats$Sales<=8,"No","Yes"))
```

Create a decision tree using all variables except the original continous `Sales` variable. 
```{r}
tree.Carseats <- tree(High~.-Sales, Carseats)
summary(tree.Carseats)
```

Plot the tree with labels.
```{r}
plot(tree.Carseats)
text(tree.Carseats,pretty=0)
```

Summarise the tree.
```{r}
tree.Carseats
```

Using the rpart package.
```{r}
library(rpart)
library(rpart.plot)

tree.Carseats.2 <- rpart(High~.-Sales, data = Carseats)

prp(tree.Carseats.2)
```

Interactively trim tree
```{r}
prp(tree.Carseats.2,snip=TRUE)$obj
```


Create train and test data sets to allow evaluation of test error.
```{r}
set.seed(2)
train <- sample(1:nrow(Carseats), 200)

tree.Carseats <- tree(High~.-Sales, Carseats, subset = train)
tree.pred <- predict(tree.Carseats, Carseats[-train,], type="class")

test.results <- table(tree.pred,Carseats[-train,"High"])
test.results
paste("Test error rate: ",100*(1-sum(diag(test.results))/length(train)),"%",sep="")
```

`rpart` test.
```{r}
tree.Carseats <- rpart(High~.-Sales, Carseats, subset = train)
tree.pred <- predict(tree.Carseats, Carseats[-train,], type="class")

test.results <- table(tree.pred,Carseats[-train,"High"])
test.results
paste("Test error rate: ",100*(1-sum(diag(test.results))/length(train)),"%",sep="")
```

Next, consider whether pruning the tree improves results; use the `cv.tree` function to perform cross-validation. Specify `prune.misclass` so that classification error used to guide cross-validation and pruning process. `dev` in `cv.carseat` is actually the misclassification error rate; lowest for `size=9` tree.
```{r}
tree.Carseats <- tree(High~.-Sales, Carseats, subset = train)

set.seed(3)

cv.carseat <- cv.tree(tree.Carseats, FUN = prune.misclass)
cv.carseat
```

`k` corresponds to the cross-complexity error.
```{r}
par(mfrow=c(1,2))
plot(cv.carseat$size, cv.carseat$dev, type="b")
plot(cv.carseat$k, cv.carseat$dev, type="b")
```

Now prune, to obtain nine node tree.
```{r}
prune.carseats <- prune.misclass(tree.Carseats, best = 9)
par(mfrow=c(1,1))
plot(prune.carseats)
text(prune.carseats,pretty = 0)
```

```{r}
tree.pred <- predict(prune.carseats, Carseats[-train,], type="class")

test.results <- table(tree.pred,Carseats[-train,"High"])
test.results
paste("Test error rate: ",100*(1-sum(diag(test.results))/length(train)),"%",sep="")
```


## Regression Trees

Fit a regression tree to the `Boston` data set.
```{r}
library(MASS)
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston)/2)

tree.boston <- tree(medv~., Boston, subset = train)
summary(tree.boston)
```

Can see from `summary` that only three variables are actually used in the tree.

```{r}
plot(tree.boston)
text(tree.boston,pretty = 0)
```

Cross validation selects the most complex tree.

```{r}
cv.boston <- cv.tree(tree.boston)
par(mfrow=c(1,2))
plot(cv.boston$size, cv.boston$dev, type="b")
plot(cv.boston$size, cv.boston$k, type="b")
```

If we did wish to prune the tree anyway, would do so as follows.

```{r}
prune.boston <- prune.tree(tree.boston, best = 5)
par(mfrow=c(1,1))
plot(prune.boston)
text(prune.boston, pretty = 0)
```

```{r}
yhat <- predict(tree.boston, newdata = Boston[-train,])

plot(yhat,Boston[-train,"medv"])
abline(0,1)
paste("Test set MSE associated with regression tree: ", round(mean((yhat-Boston[-train,"medv"])^2),3))
```


## Bagging and Random Forests

Bagging example. Bagging is where we take multiple bootstrapped samples from the same training set and build an ensemble of trees that are then averaged. Bagging uses all predictors; `mtry` states that all 13 predictors should be considered for each split of the tree.

```{r}
library(randomForest)
set.seed(1)
bag.boston <- randomForest(medv~., data = Boston, subset = train, mtry = 13, importance = T)
bag.boston
```

```{r}
yhat.bag <- predict(bag.boston, newdata = Boston[-train,])
plot(yhat.bag, Boston[-train,"medv"])
abline(0,1)
paste("Test set MSE associated with bagged tree: ", round(mean((yhat.bag-Boston[-train,"medv"])^2),2))
```

Change number of trees grown using the `ntree` argument.

```{r}
bag.boston <- randomForest(medv~., data = Boston, subset = train, mtry = 13, ntree = 25)
yhat.bag <- predict(bag.boston, newdata = Boston[-train,])
paste("Test set MSE associated with bagged tree: ", mean((yhat.bag-Boston[-train,"medv"])^2))
```

Growing a random forest proceeds in the same way, but with a smaller value for `mtry`.

```{r}
set.seed(1)
rf.boston <- randomForest(medv~., data = Boston, subset = train, mtry = 6, importance = T)
yhat.rf <- predict(rf.boston, newdata = Boston[-train,])
paste("Test set MSE associated with random forest: ", mean((yhat.rf-Boston[-train,"medv"])^2))
```

Can use `importance` function to view importance of variables used. The first, `%IncMSE`, measures the mean decrease in accuracy of the predictions on out of bag samples when that feature is excluded from the model. The second, `IncNodePurity`, measures the decrease in node impurity due to splits over that variable, over all trees; node impurity measures by training RSS in the case of regression trees, and deviance for classification trees.

```{r}
importance(rf.boston)
varImpPlot(rf.boston)
```


## Boosting

???? Use a gaussian distribution as this is a regression problem; ir performing classification, use a bernoulli distribution.

Boosting example, using `gbm` package. Plot shows relative importance. `interaction.depth` limits depth of each tree.

```{r}
library(gbm)
set.seed(1)
boost.boston <- gbm(medv~., data = Boston[-train,], distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
summary(boost.boston)
```

Can also produce *partial dependence plots*, which integrate out other variables to show the marginal effect of selected variables. Can see that median house prices are increasing with `rm`, and decreasing with `lstat`.

```{r}
par(mfrow=c(1,2))
plot(boost.boston, i="rm")
plot(boost.boston, i="lstat")
```

```{r}
yhat.boost <- predict(boost.boston, newdata = Boston[-train,], n.trees = 5000)
paste("Test set MSE associated with boosting: ", mean((yhat.boost-Boston[-train,"medv"])^2))
```


Set our shrinkage term manually to $\lambda=0.2$.
```{r}
boost.boston <- gbm(medv~., data = Boston[train,], distribution = "gaussian", n.trees = 5000, interaction.depth = 4, shrinkage = 0.2, verbose = F)
yhat.boost <- predict(boost.boston, newdata = Boston[-train,], n.trees = 5000)
paste("Test set MSE associated with boosting, shrinkage = 0.2: ", mean((yhat.boost-Boston[-train,"medv"])^2))
```

